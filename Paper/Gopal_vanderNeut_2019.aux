\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Diagrammatic Model of Reinforcement Learning (Sutton \& Barto, 1998).\relax }}{3}{figure.caption.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theoretical Background}{3}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Diagrammatic Model of Actor-Critic Architecture (Patel, 2017).\relax }}{5}{figure.caption.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Possible Metrics of TL in RL (Taylor \& Stone, 2009).\relax }}{9}{figure.caption.3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{9}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualisation of Setup.\relax }}{9}{figure.caption.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Dependency hierarchy of the studied phenomena.\relax }}{10}{figure.caption.6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Distribution of Initializations of each Game and the Game that it was Subsequently Transferred to Completed for both ACKTR and PPO2. The ‘x2’ within the cells says that each of the training runs was run twice. Cells with ‘x1’ refer to the self-transfer runs.\relax }}{11}{table.caption.5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{12}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Screenshots of the four selected games. From left to right: Q*Bert, Pong, Space Invaders and Demon Attack.\relax }}{13}{figure.caption.7}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Inferences}{13}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 500-Point rolling average of the game score plotted against the 10M timesteps. The hyphenated lines superimposed onto these graphs represents the thresholds (human level performance). Each column of graphs represents an algorithm, as PPO2 and ACKTR runs from left to right respectively.Each row of graphs represents a game as Q*Bert, Pong, Space Invaders and Demon Attack from top to bottom respectively. The titles of the graphs follow the naming convention: game\_algorithm. The legend follows the naming convention: source-game\_target-game\_number-of-timesteps\_algorithm\_run-number\relax }}{14}{figure.caption.8}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{15}{section.6}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Discussion}{15}{section.7}}
\citation{Figueredo:2009dg}
\bibcite{Figueredo:2009dg}{Figueredo and Wolf, 2009}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Acknowledgments}{16}{section.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Visual representation of a Neural Network.\relax }}{17}{figure.caption.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces (Chokmani, Khalil, Ouarda, \& Bourdages, 2007) forward propagation through a single neuron is depicted.\relax }}{17}{figure.caption.10}}
\@writefile{toc}{\contentsline {paragraph}{Forward propagation}{17}{paragraph*.11}}
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{17}{paragraph*.12}}
\@writefile{toc}{\contentsline {paragraph}{Forward Propagation}{18}{paragraph*.14}}
\@writefile{toc}{\contentsline {paragraph}{Pooling}{18}{paragraph*.15}}
\@writefile{toc}{\contentsline {paragraph}{Convolution layer and progress through the network}{18}{paragraph*.16}}
\@writefile{toc}{\contentsline {paragraph}{Backward Propagation}{18}{paragraph*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Layers of a CNN (Ma, Xiang, Du, \& Fan, 2018).\relax }}{19}{figure.caption.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces (Agarwal, 2017), the derivative of the cost function just as a normal neural network but it updates the value according to the sum of the derivatives of the cost function (shown in the middle matrix) with respect to each value that is taken into account by a specific value in the mask (first matrix)\relax }}{19}{figure.caption.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces (Gillis, 2006), creating an algorithm that maximises more precisely and efficiently using $2^{nd}$ order optimisation .\relax }}{19}{figure.caption.19}}
